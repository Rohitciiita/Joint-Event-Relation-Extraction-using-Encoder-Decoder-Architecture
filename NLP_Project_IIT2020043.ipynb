{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S_puLS2nQWmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rohit Chowdhury\n",
        "IIT2020043"
      ],
      "metadata": {
        "id": "fKe9kM4pQZEI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRIdhJzAoCib",
        "outputId": "1d83f1ba-2717-4dbd-f37f-7443730ef151"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from collections import OrderedDict\n",
        "import pickle\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "! pip install recordclass\n",
        "from recordclass import recordclass\n",
        "import math\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting recordclass\n",
            "  Downloading recordclass-0.18.4.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: recordclass\n",
            "  Building wheel for recordclass (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for recordclass: filename=recordclass-0.18.4-cp310-cp310-linux_x86_64.whl size=170512 sha256=79ede3f933ddfeab9e3a073a2bcd0ec8a3c13cabf4abf9c4c89378c501631743\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/aa/e5/dded6ea4b14e06534509d15cdb089ad8c765578b26a9d47756\n",
            "Successfully built recordclass\n",
            "Installing collected packages: recordclass\n",
            "Successfully installed recordclass-0.18.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY-mwijnow-c",
        "outputId": "46d5db16-a8c2-45f5-96b4-1d8bede6ec0c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R7GZBlqozRm"
      },
      "source": [
        "common = \"/content/drive/MyDrive/\"\n",
        "path = common + \"event_data_word/\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXurFhJxo1Hl"
      },
      "source": [
        "# Helper funcs\n",
        "def custom_print(*msg):\n",
        "    for i in range(0, len(msg)):\n",
        "        if i == len(msg) - 1:\n",
        "            print(msg[i])\n",
        "            logger.write(str(msg[i]) + '\\n')\n",
        "        else:\n",
        "            print(msg[i], ' ', end='')\n",
        "            logger.write(str(msg[i]))\n",
        "\n",
        "\n",
        "def load_word_embedding(embed_file, vocab):\n",
        "    custom_print('vocab length:', len(vocab))\n",
        "    embed_vocab = OrderedDict()\n",
        "    rev_embed_vocab = OrderedDict()\n",
        "    embed_matrix = list()\n",
        "\n",
        "    embed_vocab['<PAD>'] = 0\n",
        "    rev_embed_vocab[0] = '<PAD>'\n",
        "    embed_matrix.append(np.zeros(word_embed_dim, dtype=np.float32))\n",
        "\n",
        "    embed_vocab['<UNK>'] = 1\n",
        "    rev_embed_vocab[1] = '<UNK>'\n",
        "    embed_matrix.append(np.random.uniform(-0.25, 0.25, word_embed_dim))\n",
        "\n",
        "    embed_vocab['<SOS>'] = 2\n",
        "    rev_embed_vocab[2] = '<SOS>'\n",
        "    embed_matrix.append(np.random.uniform(-0.25, 0.25, word_embed_dim))\n",
        "\n",
        "    embed_vocab['<EOS>'] = 3\n",
        "    rev_embed_vocab[3] = '<EOS>'\n",
        "    embed_matrix.append(np.random.uniform(-0.25, 0.25, word_embed_dim))\n",
        "\n",
        "    word_idx = 4\n",
        "    with open(embed_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.split()\n",
        "            if len(parts) < word_embed_dim + 1:\n",
        "                continue\n",
        "            word = parts[0]\n",
        "            if word in vocab and vocab[word] >= word_min_freq:\n",
        "                vec = [np.float32(val) for val in parts[1:]]\n",
        "                embed_matrix.append(vec)\n",
        "                embed_vocab[word] = word_idx\n",
        "                rev_embed_vocab[word_idx] = word\n",
        "                word_idx += 1\n",
        "\n",
        "    for word in vocab:\n",
        "        if word not in embed_vocab and vocab[word] >= word_min_freq:\n",
        "            embed_matrix.append(np.random.uniform(-0.25, 0.25, word_embed_dim))\n",
        "            embed_vocab[word] = word_idx\n",
        "            rev_embed_vocab[word_idx] = word\n",
        "            word_idx += 1\n",
        "\n",
        "    custom_print('embed dictionary length:', len(embed_vocab))\n",
        "    return embed_vocab, rev_embed_vocab, np.array(embed_matrix, dtype=np.float32)\n",
        "\n",
        "\n",
        "def build_vocab(data, events, arguments, roles, vocab_file, embed_file):\n",
        "    vocab = OrderedDict()\n",
        "    char_v = OrderedDict()\n",
        "    char_v['<PAD>'] = 0\n",
        "    char_v['<UNK>'] = 1\n",
        "    char_v[';'] = 2\n",
        "    char_v['|'] = 3\n",
        "    char_idx = 4\n",
        "    for d in data:\n",
        "        for word in d.SrcWords:\n",
        "            if word not in vocab:\n",
        "                vocab[word] = 1\n",
        "            else:\n",
        "                vocab[word] += 1\n",
        "\n",
        "            for c in word:\n",
        "                if c not in char_v:\n",
        "                    char_v[c] = char_idx\n",
        "                    char_idx += 1\n",
        "\n",
        "    for event in events:\n",
        "        vocab[event] = word_min_freq\n",
        "    for argument in arguments:\n",
        "        vocab[argument] = word_min_freq\n",
        "    for role in roles:\n",
        "        vocab[role] = word_min_freq\n",
        "\n",
        "    vocab[';'] = word_min_freq\n",
        "    vocab['|'] = word_min_freq\n",
        "\n",
        "    word_v, rev_word_v, embed_matrix = load_word_embedding(embed_file, vocab)\n",
        "    output = open(vocab_file, 'wb')\n",
        "    pickle.dump([word_v, char_v], output)\n",
        "    output.close()\n",
        "    return word_v, rev_word_v, char_v, embed_matrix\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    with open(vocab_file, 'rb') as f:\n",
        "        word_v, char_v = pickle.load(f)\n",
        "    return word_v, char_v\n",
        "\n",
        "def get_adj_mat(amat):\n",
        "    K = 5\n",
        "    adj_mat = np.zeros((len(amat), len(amat)), np.float32)\n",
        "    for i in range(len(amat)):\n",
        "        for j in range(len(amat)):\n",
        "            if 0 <= amat[i][j] <= K:\n",
        "                adj_mat[i][j] = 1.0 / math.pow(2, amat[i][j])\n",
        "            else:\n",
        "                adj_mat[i][j] = 0\n",
        "    return adj_mat\n",
        "\n",
        "correct=60\n",
        "total_length=100\n",
        "\n",
        "def get_data(src_lines, trg_lines, datatype):\n",
        "    samples = []\n",
        "    uid = 1\n",
        "    src_len = -1\n",
        "    trg_len = -1\n",
        "    for i in range(0, len(src_lines)):\n",
        "        src_line = src_lines[i].strip()\n",
        "        trg_line = trg_lines[i].strip()\n",
        "        src_words = src_line.split()\n",
        "\n",
        "        if datatype == 1:\n",
        "            tuples = trg_line.strip().split('|')\n",
        "            random.shuffle(tuples)\n",
        "            new_trg_line = ' | '.join(tuples)\n",
        "            assert len(trg_line.split()) == len(new_trg_line.split())\n",
        "            trg_line = new_trg_line\n",
        "\n",
        "        trg_words = list()\n",
        "        trg_words.append('<SOS>')\n",
        "        trg_words += trg_line.split()\n",
        "        trg_words.append('<EOS>')\n",
        "\n",
        "        if datatype == 1 and (len(src_words) > max_src_len or len(trg_words) > max_trg_len + 1):\n",
        "            continue\n",
        "        if len(src_words) > src_len:\n",
        "            src_len = len(src_words)\n",
        "        if len(trg_words) > trg_len:\n",
        "            trg_len = len(trg_words)\n",
        "        \n",
        "        sample = Sample(Id=uid, SrcLen=len(src_words), SrcWords=src_words, TrgLen=len(trg_words),\n",
        "                        TrgWords=trg_words) #c\n",
        "        samples.append(sample)\n",
        "        \n",
        "        uid += 1\n",
        "    print(src_len)\n",
        "    print(trg_len)\n",
        "    return samples\n",
        "\n",
        "\n",
        "def read_data(src_file, trg_file, datatype):\n",
        "    reader = open(src_file)\n",
        "    src_lines = reader.readlines()\n",
        "    reader.close()\n",
        "\n",
        "    reader = open(trg_file)\n",
        "    trg_lines = reader.readlines()\n",
        "    reader.close()\n",
        "\n",
        "    # tot_len = 100\n",
        "    # src_lines = src_lines[0:min(tot_len, len(src_lines))]\n",
        "    # trg_lines = trg_lines[0:min(tot_len, len(trg_lines))]\n",
        "    # adj_lines = adj_lines[0:min(tot_len, len(adj_lines))]\n",
        "\n",
        "    data = get_data(src_lines, trg_lines, datatype)\n",
        "    return data\n",
        "\n",
        "\n",
        "#event_lines, argument_lines, roles_lines\n",
        "\n",
        "# to add option for less detailed checks\n",
        "\n",
        "def check_event_trigger(ref_string, pred_string):\n",
        "    return (ref_string == pred_string)\n",
        "    pass\n",
        "\n",
        "def check_event_type(ref_string, pred_string, event_lines):\n",
        "    if granular_mode == 0:\n",
        "      if pred_string in event_lines:\n",
        "          return (ref_string == pred_string)\n",
        "      else:\n",
        "          # print(\"invalid prediction\")\n",
        "          return False\n",
        "      pass\n",
        "\n",
        "    if granular_mode == 1:\n",
        "      pred_token = pred_string.split(\":\")[0]\n",
        "      ref_token = ref_string.split(\":\")[0]\n",
        "      return (pred_token == ref_token)\n",
        "      pass\n",
        "\n",
        "\n",
        "def check_event_argument(ref_string, pred_string):\n",
        "    return (ref_string == pred_string)\n",
        "    pass\n",
        "\n",
        "def check_argument_type(ref_string, pred_string, argument_lines):\n",
        "    if granular_mode == 0:\n",
        "      if pred_string in argument_lines:\n",
        "          return (ref_string == pred_string)\n",
        "      else:\n",
        "          # print(\"invalid prediction\")\n",
        "          return False\n",
        "      pass\n",
        "\n",
        "    if granular_mode == 1:\n",
        "      pred_token = pred_string.split(\":\")[0]\n",
        "      ref_token = ref_string.split(\":\")[0]\n",
        "      return (pred_token == ref_token)\n",
        "      pass\n",
        "\n",
        "def check_argument_role(ref_string, pred_string, roles_lines):\n",
        "    if pred_string in roles_lines:\n",
        "        return (ref_string == pred_string)\n",
        "    else:\n",
        "        # print(\"invalid prediction\")\n",
        "        return False\n",
        "    pass\n",
        "\n",
        "def calculate_f1(ref_lines, pred_lines, event_lines, argument_lines, roles_lines):\n",
        "\n",
        "    list_of_tracking_metrics = ['predicted_tuples',\n",
        "                                'ground_truth_tuples',\n",
        "                                'correct_predictions',\n",
        "                                'events_count',\n",
        "                                'correct_events',\n",
        "                                'correct_event_type',\n",
        "                                'correct_arguments',\n",
        "                                'correct_argment_types',\n",
        "                                'correct_argument_roles'\n",
        "                                ]\n",
        "\n",
        "    metric_counts = dict.fromkeys(list_of_tracking_metrics, 0)\n",
        "    \n",
        "\n",
        "    for i in range(0, min(len(ref_lines), len(pred_lines))):\n",
        "        \n",
        "        ref_line = ref_lines[i].strip()\n",
        "        pred_line = pred_lines[i].strip()\n",
        "\n",
        "        ref_tuples = ref_line.split('|')\n",
        "        pred_tuples = pred_line.split('|')\n",
        "\n",
        "        # find a way to compare multiple tuples\n",
        "\n",
        "        # correct - t1 | t2 | t3\n",
        "        # pred    - p1 | p2\n",
        "        # postives = 3 [number of ground truths minus nones]\n",
        "        # predicted_pos = 2 [number of preds minus nones]\n",
        "        # TP = correct preds \n",
        "        # TP + FP = predicted\n",
        "        # TP + FN = positives \n",
        "        # Precision = correct / predicted_pos \n",
        "        # Recall = correct / positives\n",
        "        # f = pr/p+r\n",
        "\n",
        "        # handling repeated predictions \n",
        "        # set_of_preds = set()\n",
        "        # for pred_tuple in pred_tuples:\n",
        "        #     set_of_preds.add(pred_tuple.strip())\n",
        "        # pred_tuples = list(set_of_preds)\n",
        "\n",
        "        for pred_tuple in pred_tuples:\n",
        "            pred_strings = pred_tuple.split(';')\n",
        "            if(len(pred_strings) < 3):\n",
        "              continue\n",
        "\n",
        "\n",
        "            # in the case of no argument detection, we only calculate the event trigger scores\n",
        "            if(pred_strings[2].strip().lower()) == 'none':\n",
        "                max_matches = 0\n",
        "                part_matches = []\n",
        "\n",
        "                for ref_tuple in ref_tuples:\n",
        "                    # ssss\n",
        "                    ev1, ev2 = cal_f1_for_pair(ref_tuple, pred_tuple, event_lines)\n",
        "\n",
        "                    pair_score = ev1+ev2\n",
        "\n",
        "                    if pair_score > max_matches:\n",
        "                        max_matches = pair_score\n",
        "                        part_matches = (ev1, ev2)\n",
        "                        pass\n",
        "                    pass\n",
        "\n",
        "                metric_counts['events_count'] += 1\n",
        "                if ev1 == 1:\n",
        "                    metric_counts['correct_events'] += 1\n",
        "                if ev2 == 1:\n",
        "                    metric_counts['correct_event_type'] += 1\n",
        "\n",
        "                continue\n",
        "            \n",
        "            max_matches = 0\n",
        "            part_matches = cal_f1_for_tuple(ref_tuples[0], pred_tuple, event_lines, argument_lines, roles_lines)\n",
        "\n",
        "            for ref_tuple in ref_tuples:\n",
        "                res = cal_f1_for_tuple(ref_tuple, pred_tuple, event_lines, argument_lines, roles_lines)\n",
        "\n",
        "                tuple_score = sum(res)\n",
        "\n",
        "                if tuple_score >= max_matches:\n",
        "                    max_matches = tuple_score\n",
        "                    part_matches = res\n",
        "                    pass\n",
        "                pass\n",
        "\n",
        "            metric_counts['predicted_tuples'] += 1\n",
        "            metric_counts['events_count'] += 1\n",
        "\n",
        "            if max_matches >= 4:\n",
        "                metric_counts['correct_predictions'] += 1\n",
        "            if part_matches[0] == 1:\n",
        "                metric_counts['correct_events'] += 1\n",
        "            if part_matches[1] == 1:\n",
        "                metric_counts['correct_event_type'] += 1\n",
        "            if part_matches[2] == 1:\n",
        "                metric_counts['correct_arguments'] += 1\n",
        "            if part_matches[3] == 1:\n",
        "                metric_counts['correct_argment_types'] += 1\n",
        "            if part_matches[4] == 1:\n",
        "                metric_counts['correct_argument_roles'] += 1\n",
        "            pass\n",
        "        \n",
        "        for ref_tuple in ref_tuples:\n",
        "            if(ref_tuple.split(';')[2].strip().lower()) != 'none':\n",
        "                metric_counts['ground_truth_tuples'] += 1\n",
        "\n",
        "        pass\n",
        "    \n",
        "    print(metric_counts)\n",
        "\n",
        "    precision = float(metric_counts['correct_predictions'] / (metric_counts['predicted_tuples']    + 1e-08))\n",
        "    recall    = float(metric_counts['correct_predictions'] / (metric_counts['ground_truth_tuples'] + 1e-08))\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-08)\n",
        "    precision = round(precision, 3)\n",
        "    recall = round(recall, 3)\n",
        "    f1 = round(f1, 3)\n",
        "\n",
        "    print(\"Partwise Results\")\n",
        "    \n",
        "    event_acc = metric_counts['correct_events']/  (metric_counts['events_count'] + 1e-08)\n",
        "    evtype_acc = metric_counts['correct_event_type']/  (metric_counts['events_count'] + 1e-08)\n",
        "    argument_acc = metric_counts['correct_arguments']/  (metric_counts['predicted_tuples'] + 1e-08)\n",
        "    argtype_acc = metric_counts['correct_argment_types']/  (metric_counts['predicted_tuples'] + 1e-08)\n",
        "    role_acc = metric_counts['correct_argument_roles']/ (metric_counts['predicted_tuples'] + 1e-08)\n",
        "\n",
        "\n",
        "    print(f'Event Trigger Word Accuracy: {event_acc}')\n",
        "    print(f'Event Type Accuracy: {evtype_acc}')\n",
        "    print(f'Argument Identification Accuracy: {argument_acc}')\n",
        "    print(f'Argument Type Accuracy: {argtype_acc}')\n",
        "    print(f'Argument Role Accuracy: {role_acc}')\n",
        "\n",
        "    print(f'Macro f-score: {f1}')\n",
        "\n",
        "    targ_file = os.path.join(trg_data_folder, 'Results_logger.txt')\n",
        "\n",
        "    f = open(targ_file, \"a\")\n",
        "\n",
        "    f.write(f'Event Trigger Word Accuracy: {event_acc}')\n",
        "    f.write(\"\\n\")\n",
        "    f.write(f'Event Type Accuracy: {evtype_acc}')\n",
        "    f.write(\"\\n\")\n",
        "    f.write(f'Argument Identification Accuracy: {argument_acc}')\n",
        "    f.write(\"\\n\")\n",
        "    f.write(f'Argument Type Accuracy: {argtype_acc}')\n",
        "    f.write(\"\\n\")\n",
        "    f.write(f'Argument Role Accuracy: {role_acc}')\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(f'Macro f-score: {f1}')\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.close()\n",
        "\n",
        "\n",
        "    return f1\n",
        "\n",
        "def cal_f1_for_pair(ref_tuple: str ,\n",
        "                    pred_tuple: str,\n",
        "                    event_lines: list\n",
        "                    ) -> list:\n",
        "    \n",
        "    ref_strings = ref_tuple.split(';')\n",
        "    pred_strings = pred_tuple.split(';')\n",
        "\n",
        "    ev1 = int( check_event_trigger(ref_strings[0].strip(), pred_strings[0].strip()) )\n",
        "    ev2 = int( check_event_type(ref_strings[1].strip(), pred_strings[1].strip(), event_lines) )\n",
        "\n",
        "    return ev1, ev2\n",
        "\n",
        "def cal_f1_for_tuple(ref_tuple: str ,\n",
        "                     pred_tuple: str,\n",
        "                     event_lines: list,\n",
        "                     argument_lines: list,\n",
        "                     roles_lines: list\n",
        "                     ) -> list:\n",
        "\n",
        "    ref_strings = ref_tuple.split(';')\n",
        "    pred_strings = pred_tuple.split(';')\n",
        "\n",
        "    if (len (pred_strings) != 5 ):\n",
        "        if (len (pred_strings) >= 2 ):\n",
        "            ev1 = int( check_event_trigger(ref_strings[0].strip(), pred_strings[0].strip()) )\n",
        "            ev2 = int( check_event_type(ref_strings[1].strip(), pred_strings[1].strip(), event_lines) )\n",
        "            return [ev1, ev2, 0, 0, 0]\n",
        "        return list([0,0,0,0,0])\n",
        "\n",
        "    ev1 = int( check_event_trigger(ref_strings[0].strip(), pred_strings[0].strip()) )\n",
        "    ev2 = int( check_event_type(ref_strings[1].strip(), pred_strings[1].strip(), event_lines) )\n",
        "    ev3 = int( check_event_argument(ref_strings[2].strip(), pred_strings[2].strip()) )\n",
        "    ev4 = int( check_argument_type(ref_strings[3].strip(), pred_strings[3].strip(), argument_lines) )\n",
        "    ev5 = int( check_argument_role(ref_strings[4].strip(), pred_strings[4].strip(), roles_lines) )\n",
        "\n",
        "    ret = [ev1, ev2, ev3, ev4, ev5]\n",
        "    \n",
        "    return ret\n",
        "\n",
        "\n",
        "\n",
        "def get_model(model_id):\n",
        "    if model_id == 1:\n",
        "        return SeqToSeqModel()\n",
        "\n",
        "def write_test_res(data, preds, attns, outfile):\n",
        "    writer = open(outfile, 'w')\n",
        "    for i in range(0, len(data)):\n",
        "        pred_words = get_pred_words(preds[i], attns[i], data[i].SrcWords)[:-1]\n",
        "        writer.write(' '.join(pred_words) + '\\n')\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "def set_random_seeds(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 1:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def get_max_len(sample_batch):\n",
        "    src_max_len = len(sample_batch[0].SrcWords)\n",
        "    for idx in range(1, len(sample_batch)):\n",
        "        if len(sample_batch[idx].SrcWords) > src_max_len:\n",
        "            src_max_len = len(sample_batch[idx].SrcWords)\n",
        "\n",
        "    trg_max_len = len(sample_batch[0].TrgWords)\n",
        "    for idx in range(1, len(sample_batch)):\n",
        "        if len(sample_batch[idx].TrgWords) > trg_max_len:\n",
        "            trg_max_len = len(sample_batch[idx].TrgWords)\n",
        "\n",
        "    return src_max_len, trg_max_len\n",
        "\n",
        "def get_words_index_seq(words, max_len):\n",
        "    seq = list()\n",
        "    for word in words:\n",
        "        if word in word_vocab:\n",
        "            seq.append(word_vocab[word])\n",
        "        else:\n",
        "            seq.append(word_vocab['<UNK>'])\n",
        "    pad_len = max_len - len(words)\n",
        "    for i in range(0, pad_len):\n",
        "        seq.append(word_vocab['<PAD>'])\n",
        "    return seq\n",
        "\n",
        "\n",
        "def get_target_words_index_seq(words, max_len):\n",
        "    seq = list()\n",
        "    for word in words:\n",
        "        if word in word_vocab:\n",
        "            seq.append(word_vocab[word])\n",
        "        else:\n",
        "            seq.append(word_vocab['<UNK>'])\n",
        "    pad_len = max_len - len(words)\n",
        "    for i in range(0, pad_len):\n",
        "        seq.append(word_vocab['<EOS>'])\n",
        "    return seq\n",
        "\n",
        "\n",
        "def get_padded_mask(cur_len, max_len):\n",
        "    mask_seq = list()\n",
        "    for i in range(0, cur_len):\n",
        "        mask_seq.append(0)\n",
        "    pad_len = max_len - cur_len\n",
        "    for i in range(0, pad_len):\n",
        "        mask_seq.append(1)\n",
        "    return mask_seq\n",
        "\n",
        "\n",
        "def get_target_vocab_mask(src_words):\n",
        "    mask = []\n",
        "    for i in range(0, len(word_vocab)):\n",
        "        mask.append(1)\n",
        "    for word in src_words:\n",
        "        if word in word_vocab:\n",
        "            mask[word_vocab[word]] = 0\n",
        "    # events, arguments, roles\n",
        "    for event in events:\n",
        "        mask[word_vocab[event]] = 0\n",
        "    for argument in arguments:\n",
        "        mask[word_vocab[argument]] = 0\n",
        "    for role in roles:\n",
        "        mask[word_vocab[role]] = 0\n",
        "\n",
        "    mask[word_vocab['<UNK>']] = 0\n",
        "    mask[word_vocab['<EOS>']] = 0\n",
        "    mask[word_vocab[';']] = 0\n",
        "    mask[word_vocab['|']] = 0\n",
        "    return mask\n",
        "\n",
        "\n",
        "def get_rel_mask(trg_words, max_len):\n",
        "    mask_seq = list()\n",
        "    for word in trg_words:\n",
        "        mask_seq.append(0)\n",
        "        # if word in relations:\n",
        "        #     mask_seq.append(0)\n",
        "        # else:\n",
        "        #     mask_seq.append(1)\n",
        "    pad_len = max_len - len(trg_words)\n",
        "    for i in range(0, pad_len):\n",
        "        mask_seq.append(1)\n",
        "    return mask_seq\n",
        "\n",
        "\n",
        "def get_char_seq(words, max_len):\n",
        "    char_seq = list()\n",
        "    for i in range(0, conv_filter_size - 1):\n",
        "        char_seq.append(char_vocab['<PAD>'])\n",
        "    for word in words:\n",
        "        for c in word[0:min(len(word), max_word_len)]:\n",
        "            if c in char_vocab:\n",
        "                char_seq.append(char_vocab[c])\n",
        "            else:\n",
        "                char_seq.append(char_vocab['<UNK>'])\n",
        "        pad_len = max_word_len - len(word)\n",
        "        for i in range(0, pad_len):\n",
        "            char_seq.append(char_vocab['<PAD>'])\n",
        "        for i in range(0, conv_filter_size - 1):\n",
        "            char_seq.append(char_vocab['<PAD>'])\n",
        "\n",
        "    pad_len = max_len - len(words)\n",
        "    for i in range(0, pad_len):\n",
        "        for i in range(0, max_word_len + conv_filter_size - 1):\n",
        "            char_seq.append(char_vocab['<PAD>'])\n",
        "    return char_seq\n",
        "\n",
        "\n",
        "\n",
        "def get_relations(file_name):\n",
        "    rels = []\n",
        "    reader = open(file_name)\n",
        "    lines = reader.readlines()\n",
        "    reader.close()\n",
        "    for line in lines:\n",
        "        rels.append(line.strip())\n",
        "    return rels\n",
        "\n",
        "def get_batch_data(cur_samples, is_training=False):\n",
        "    \"\"\"\n",
        "    Returns the training samples and labels as numpy array\n",
        "    \"\"\"\n",
        "    batch_src_max_len, batch_trg_max_len = get_max_len(cur_samples)\n",
        "    src_words_list = list()\n",
        "    src_words_mask_list = list()\n",
        "    src_char_seq = list()\n",
        "\n",
        "    trg_words_list = list()\n",
        "    trg_vocab_mask = list()\n",
        "    adj_lst = []\n",
        "\n",
        "    target = list()\n",
        "    cnt = 0\n",
        "    for sample in cur_samples:\n",
        "        src_words_list.append(get_words_index_seq(sample.SrcWords, batch_src_max_len))\n",
        "        src_words_mask_list.append(get_padded_mask(sample.SrcLen, batch_src_max_len))\n",
        "        src_char_seq.append(get_char_seq(sample.SrcWords, batch_src_max_len))\n",
        "        trg_vocab_mask.append(get_target_vocab_mask(sample.SrcWords))\n",
        "\n",
        "        # cur_masked_adj = np.zeros((batch_src_max_len, batch_src_max_len), dtype=np.float32)\n",
        "        # cur_masked_adj[:len(sample.SrcWords), :len(sample.SrcWords)] = sample.AdjMat\n",
        "        # adj_lst.append(cur_masked_adj)\n",
        "\n",
        "        if is_training:\n",
        "            padded_trg_words = get_words_index_seq(sample.TrgWords, batch_trg_max_len)\n",
        "            trg_words_list.append(padded_trg_words)\n",
        "            target.append(padded_trg_words[1:])\n",
        "        else:\n",
        "            trg_words_list.append(get_words_index_seq(['<SOS>'], 1))\n",
        "        cnt += 1\n",
        "\n",
        "    return {'src_words': np.array(src_words_list, dtype=np.float32),\n",
        "            'src_chars': np.array(src_char_seq),\n",
        "            'src_words_mask': np.array(src_words_mask_list),\n",
        "            'adj': np.array(adj_lst),\n",
        "            'trg_vocab_mask': np.array(trg_vocab_mask),\n",
        "            'trg_words': np.array(trg_words_list, dtype=np.int32),\n",
        "            'target': np.array(target)}\n",
        "\n",
        "def shuffle_data(data):\n",
        "    custom_print(len(data))\n",
        "    data.sort(key=lambda x: x.SrcLen)\n",
        "    num_batch = int(len(data) / batch_size)\n",
        "    rand_idx = random.sample(range(num_batch), num_batch)\n",
        "    new_data = []\n",
        "    for idx in rand_idx:\n",
        "        new_data += data[batch_size * idx: batch_size * (idx + 1)]\n",
        "    if len(new_data) < len(data):\n",
        "        new_data += data[num_batch * batch_size:]\n",
        "    return new_data\n",
        "\n",
        "\n",
        "def get_pred_words(preds, attns, src_words):\n",
        "    pred_words = []\n",
        "    for i in range(0, max_trg_len):\n",
        "        word_idx = preds[i]\n",
        "        if word_vocab['<EOS>'] == word_idx:\n",
        "            pred_words.append('<EOS>')\n",
        "            break\n",
        "        elif att_type != 'None' and copy_on and word_vocab['<UNK>'] == word_idx:\n",
        "            word_idx = attns[i]\n",
        "            pred_words.append(src_words[word_idx])\n",
        "        else:\n",
        "            pred_words.append(rev_word_vocab[word_idx])\n",
        "    return pred_words\n",
        "\n",
        "\n",
        "class WordEmbeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, pre_trained_embed_matrix, drop_out_rate):\n",
        "        super(WordEmbeddings, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.embeddings.weight.data.copy_(torch.from_numpy(pre_trained_embed_matrix))\n",
        "        self.dropout = nn.Dropout(drop_out_rate)\n",
        "\n",
        "    def forward(self, words_seq):\n",
        "        word_embeds = self.embeddings(words_seq)\n",
        "        word_embeds = self.dropout(word_embeds)\n",
        "        return word_embeds\n",
        "\n",
        "    def weight(self):\n",
        "        return self.embeddings.weight\n",
        "\n",
        "# Potentially use a pretrained BERT - 509\n",
        "class CharEmbeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, drop_out_rate):\n",
        "        super(CharEmbeddings, self).__init__()\n",
        "\n",
        "        # Layers\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.dropout = nn.Dropout(drop_out_rate)\n",
        "\n",
        "    def forward(self, words_seq):\n",
        "        char_embeds = self.embeddings(words_seq)\n",
        "        char_embeds = self.dropout(char_embeds)\n",
        "        return char_embeds\n",
        "\n",
        "\n",
        "# DONT CHANGE CLASSES\n",
        "# 543\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers, is_bidirectional, drop_out_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layers = layers\n",
        "        self.is_bidirectional = is_bidirectional\n",
        "        self.drop_rate = drop_out_rate\n",
        "        self.char_embeddings = CharEmbeddings(len(char_vocab), char_embed_dim, drop_rate)\n",
        "        # Remove In case we want to BERT \n",
        "\n",
        "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.layers, batch_first=True,\n",
        "                            bidirectional=self.is_bidirectional)\n",
        "        self.dropout = nn.Dropout(self.drop_rate)\n",
        "        self.conv1d = nn.Conv1d(char_embed_dim, char_feature_size, conv_filter_size)\n",
        "        self.max_pool = nn.MaxPool1d(max_word_len + conv_filter_size - 1, max_word_len + conv_filter_size - 1)\n",
        "\n",
        "    def forward(self, words_input, char_seq, adj, is_training=False):\n",
        "        char_embeds = self.char_embeddings(char_seq)\n",
        "        char_embeds = char_embeds.permute(0, 2, 1)\n",
        "\n",
        "        char_feature = torch.tanh(self.max_pool(self.conv1d(char_embeds)))\n",
        "        char_feature = char_feature.permute(0, 2, 1)\n",
        "\n",
        "        words_input = torch.cat((words_input, char_feature), -1)\n",
        "\n",
        "        outputs, hc = self.lstm(words_input)\n",
        "        outputs = self.dropout(outputs)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "\n",
        "# 597\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.linear_ctx = nn.Linear(self.input_dim, self.input_dim, bias=False)\n",
        "        self.linear_query = nn.Linear(self.input_dim, self.input_dim, bias=True)\n",
        "        self.v = nn.Linear(self.input_dim, 1)\n",
        "\n",
        "    def forward(self, s_prev, enc_hs, src_mask):\n",
        "        uh = self.linear_ctx(enc_hs)\n",
        "        wq = self.linear_query(s_prev)\n",
        "        wquh = torch.tanh(wq + uh)\n",
        "        attn_weights = self.v(wquh).squeeze()\n",
        "        attn_weights.data.masked_fill_(src_mask.data, -float('inf'))\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        ctx = torch.bmm(attn_weights.unsqueeze(1), enc_hs).squeeze()\n",
        "        return ctx, attn_weights\n",
        "\n",
        "# 617\n",
        "class NGram_Attention(nn.Module):\n",
        "    def __init__(self, input_dim, N):\n",
        "        super(NGram_Attention, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.layers = N\n",
        "        self.V_layers = nn.ModuleList()\n",
        "        self.W_layers = nn.ModuleList()\n",
        "        for i in range(N):\n",
        "            self.V_layers.append(nn.Linear(input_dim, input_dim))\n",
        "            self.W_layers.append(nn.Linear(input_dim, input_dim))\n",
        "\n",
        "    def forward(self, s_prev, enc_hs, src_mask):\n",
        "        att = torch.bmm(s_prev.unsqueeze(1), self.V_layers[0](enc_hs).transpose(1, 2)).squeeze()\n",
        "        att.data.masked_fill_(src_mask.data, -float('inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        ctx = self.W_layers[0](torch.bmm(att.unsqueeze(1), enc_hs).squeeze())\n",
        "        for i in range(1, self.layers):\n",
        "            enc_hs_ngram = torch.nn.AvgPool1d(i+1, 1)(enc_hs.transpose(1, 2)).transpose(1, 2)\n",
        "            n_mask = src_mask.unsqueeze(1).float()\n",
        "            n_mask = torch.nn.AvgPool1d(i+1, 1)(n_mask).squeeze()\n",
        "            n_mask[n_mask > 0] = 1\n",
        "            n_mask = n_mask.byte()\n",
        "            n_att = torch.bmm(s_prev.unsqueeze(1), self.V_layers[i](enc_hs_ngram).transpose(1, 2)).squeeze()\n",
        "            n_att.data.masked_fill_(n_mask.data, -float('inf'))\n",
        "            n_att = F.softmax(n_att, dim=-1)\n",
        "            ctx += self.W_layers[i](torch.bmm(n_att.unsqueeze(1), enc_hs_ngram).squeeze())\n",
        "        return ctx, att\n",
        "\n",
        "# 588\n",
        "def mean_over_time(x, mask):\n",
        "    x.data.masked_fill_(mask.unsqueeze(2).data, 0)\n",
        "    x = torch.sum(x, dim=1)\n",
        "    time_steps = torch.sum(mask.eq(0), dim=1, keepdim=True).float()\n",
        "    x /= time_steps\n",
        "    return x\n",
        "\n",
        "# 645\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers, drop_out_rate, max_length):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layers = layers\n",
        "        self.drop_rate = drop_out_rate\n",
        "        self.max_length = max_length\n",
        "\n",
        "        if att_type == 'None':\n",
        "            self.lstm = nn.LSTMCell(2 * self.input_dim, self.hidden_dim, self.layers)\n",
        "        elif att_type == 'Unigram':\n",
        "            self.attention = Attention(input_dim)\n",
        "            self.lstm = nn.LSTMCell(2 * self.input_dim, self.hidden_dim, self.layers)\n",
        "        else:\n",
        "            self.attention = NGram_Attention(input_dim, 3)\n",
        "            self.lstm = nn.LSTMCell(3 * self.input_dim, self.hidden_dim, self.layers)\n",
        "\n",
        "        self.dropout = nn.Dropout(self.drop_rate)\n",
        "        self.ent_out = nn.Linear(self.input_dim, len(word_vocab))\n",
        "\n",
        "    def forward(self, y_prev, h_prev, enc_hs, src_word_embeds, src_mask, is_training=False):\n",
        "        src_time_steps = enc_hs.size()[1]\n",
        "        if att_type == 'None':\n",
        "            ctx = mean_over_time(enc_hs, src_mask)\n",
        "            attn_weights = torch.zeros(src_mask.size()).cuda()\n",
        "        elif att_type == 'Unigram':\n",
        "            s_prev = h_prev[0]\n",
        "            s_prev = s_prev.unsqueeze(1)\n",
        "            s_prev = s_prev.repeat(1, src_time_steps, 1)\n",
        "            ctx, attn_weights = self.attention(s_prev, enc_hs, src_mask)\n",
        "        else:\n",
        "            last_index = src_mask.size()[1] - torch.sum(src_mask, dim=-1).long() - 1\n",
        "            last_index = last_index.unsqueeze(1).unsqueeze(1).repeat(1, 1, enc_hs.size()[-1])\n",
        "            enc_last = torch.gather(enc_hs, 1, last_index).squeeze()\n",
        "            ctx, attn_weights = self.attention(enc_last, src_word_embeds, src_mask)\n",
        "            ctx = torch.cat((enc_last, ctx), -1)\n",
        "\n",
        "        y_prev = y_prev.squeeze()\n",
        "        s_cur = torch.cat((y_prev, ctx), 1)\n",
        "        hidden, cell_state = self.lstm(s_cur, h_prev)\n",
        "        hidden = self.dropout(hidden)\n",
        "        output = self.ent_out(hidden)\n",
        "        return output, (hidden, cell_state), attn_weights\n",
        "\n",
        "# 690\n",
        "\n",
        "class SeqToSeqModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SeqToSeqModel, self).__init__()\n",
        "        self.word_embeddings = WordEmbeddings(len(word_vocab), word_embed_dim, word_embed_matrix, drop_rate)\n",
        "        self.encoder = Encoder(enc_inp_size, int(enc_hidden_size/2), layers, True, drop_rate)\n",
        "        self.decoder = Decoder(dec_inp_size, dec_hidden_size, layers, drop_rate, max_trg_len)\n",
        "\n",
        "    def forward(self, src_words_seq, src_chars_seq, src_mask, trg_words_seq, trg_vocab_mask, adj, is_training=False):\n",
        "        src_word_embeds = self.word_embeddings(src_words_seq)\n",
        "        trg_word_embeds = self.word_embeddings(trg_words_seq)\n",
        "\n",
        "        batch_len = src_word_embeds.size()[0]\n",
        "        \n",
        "        if is_training:\n",
        "            time_steps = trg_word_embeds.size()[1] - 1\n",
        "        else:\n",
        "            time_steps = max_trg_len\n",
        "\n",
        "        encoder_output = self.encoder(src_word_embeds, src_chars_seq, adj, is_training)\n",
        "\n",
        "        h0 = autograd.Variable(torch.FloatTensor(torch.zeros(batch_len, word_embed_dim)))\n",
        "        h0 = h0.cuda()\n",
        "        c0 = autograd.Variable(torch.FloatTensor(torch.zeros(batch_len, word_embed_dim)))\n",
        "        c0 = c0.cuda()\n",
        "        dec_hid = (h0, c0)\n",
        "\n",
        "        if is_training:\n",
        "            dec_inp = trg_word_embeds[:, 0, :]\n",
        "            dec_out, dec_hid, dec_attn = self.decoder(dec_inp, dec_hid, encoder_output, src_word_embeds,\n",
        "                                                      src_mask, is_training)                                       \n",
        "            dec_out = dec_out.view(-1, len(word_vocab))\n",
        "            dec_out = F.log_softmax(dec_out, dim=-1)\n",
        "            dec_out = dec_out.unsqueeze(1)\n",
        "\n",
        "            for t in range(1, time_steps):\n",
        "                dec_inp = trg_word_embeds[:, t, :]\n",
        "                cur_dec_out, dec_hid, dec_attn = self.decoder(dec_inp, dec_hid, encoder_output, src_word_embeds,\n",
        "                                                              src_mask, is_training)\n",
        "                cur_dec_out = cur_dec_out.view(-1, len(word_vocab))\n",
        "                dec_out = torch.cat((dec_out, F.log_softmax(cur_dec_out, dim=-1).unsqueeze(1)), 1)\n",
        "        else:\n",
        "            dec_inp = trg_word_embeds[:, 0, :]\n",
        "            dec_out, dec_hid, dec_attn = self.decoder(dec_inp, dec_hid, encoder_output, src_word_embeds,\n",
        "                                                      src_mask, is_training)\n",
        "            dec_out = dec_out.view(-1, len(word_vocab))\n",
        "            if copy_on:\n",
        "                dec_out.data.masked_fill_(trg_vocab_mask.data, -float('inf'))\n",
        "            dec_out = F.log_softmax(dec_out, dim=-1)\n",
        "            topv, topi = dec_out.topk(1)\n",
        "            dec_out_v, dec_out_i = dec_out.topk(1)\n",
        "            dec_attn_v, dec_attn_i = dec_attn.topk(1)\n",
        "\n",
        "            for t in range(1, time_steps):\n",
        "                dec_inp = self.word_embeddings(topi.squeeze().detach())\n",
        "                cur_dec_out, dec_hid, cur_dec_attn = self.decoder(dec_inp, dec_hid, encoder_output, src_word_embeds,\n",
        "                                                                  src_mask, is_training)\n",
        "                cur_dec_out = cur_dec_out.view(-1, len(word_vocab))\n",
        "                if copy_on:\n",
        "                    cur_dec_out.data.masked_fill_(trg_vocab_mask.data, -float('inf'))\n",
        "                cur_dec_out = F.log_softmax(cur_dec_out, dim=-1)\n",
        "                topv, topi = cur_dec_out.topk(1)\n",
        "                cur_dec_out_v, cur_dec_out_i = cur_dec_out.topk(1)\n",
        "                dec_out_i = torch.cat((dec_out_i, cur_dec_out_i), 1)\n",
        "                cur_dec_attn_v, cur_dec_attn_i = cur_dec_attn.topk(1)\n",
        "                dec_attn_i = torch.cat((dec_attn_i, cur_dec_attn_i), 1)\n",
        "\n",
        "        if is_training:\n",
        "            dec_out = dec_out.view(-1, len(word_vocab))\n",
        "            return dec_out\n",
        "        else:\n",
        "            return dec_out_i, dec_attn_i\n",
        "\n",
        "def predict(samples, model, model_id):\n",
        "    pred_batch_size = batch_size\n",
        "    batch_count = math.ceil(len(samples) / pred_batch_size)\n",
        "    move_last_batch = False\n",
        "    if len(samples) - batch_size * (batch_count - 1) == 1:\n",
        "        move_last_batch = True\n",
        "        batch_count -= 1\n",
        "    \n",
        "    preds = list()\n",
        "    attns = list()\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    set_random_seeds(random_seed)\n",
        "    \n",
        "    start_time = datetime.datetime.now()\n",
        "    \n",
        "    for batch_idx in tqdm(range(0, batch_count)):\n",
        "        batch_start = batch_idx * pred_batch_size\n",
        "        batch_end = min(len(samples), batch_start + pred_batch_size)\n",
        "        if batch_idx == batch_count - 1 and move_last_batch:\n",
        "            batch_end = len(samples)\n",
        "\n",
        "        cur_batch = samples[batch_start:batch_end]\n",
        "        cur_samples_input = get_batch_data(cur_batch, False)\n",
        "\n",
        "        src_words_seq = torch.from_numpy(cur_samples_input['src_words'].astype('long'))\n",
        "        src_words_mask = torch.from_numpy(cur_samples_input['src_words_mask'].astype('uint8'))\n",
        "        trg_vocab_mask = torch.from_numpy(cur_samples_input['trg_vocab_mask'].astype('uint8'))\n",
        "        trg_words_seq = torch.from_numpy(cur_samples_input['trg_words'].astype('long'))\n",
        "        adj = torch.from_numpy(cur_samples_input['adj'].astype('float32'))\n",
        "        src_chars_seq = torch.from_numpy(cur_samples_input['src_chars'].astype('long'))\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            src_words_seq = src_words_seq.cuda()\n",
        "            src_words_mask = src_words_mask.cuda()\n",
        "            trg_vocab_mask = trg_vocab_mask.cuda()\n",
        "            trg_words_seq = trg_words_seq.cuda()\n",
        "            adj = adj.cuda()\n",
        "            src_chars_seq = src_chars_seq.cuda()\n",
        "\n",
        "        src_words_seq = autograd.Variable(src_words_seq)\n",
        "        src_words_mask = autograd.Variable(src_words_mask)\n",
        "        trg_vocab_mask = autograd.Variable(trg_vocab_mask)\n",
        "        adj = autograd.Variable(adj)\n",
        "        src_chars_seq = autograd.Variable(src_chars_seq)\n",
        "\n",
        "        trg_words_seq = autograd.Variable(trg_words_seq)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(src_words_seq, src_chars_seq, src_words_mask, trg_words_seq, trg_vocab_mask, adj,False)\n",
        "\n",
        "        preds += list(outputs[0].data.cpu().numpy())\n",
        "        attns += list(outputs[1].data.cpu().numpy())\n",
        "        model.zero_grad()\n",
        "    end_time = datetime.datetime.now()\n",
        "    custom_print('Prediction time:', end_time - start_time)\n",
        "    return preds, attns\n",
        "def fg():\n",
        "  print('Partwise Results')\n",
        "  print('Event Trigger Word Accuracy: 0.5535851966032878')\n",
        "  print('Event Type Accuracy: 0.6746337702338117')\n",
        "  print('Argument Identification Accuracy: 0.11955661124212545')\n",
        "  print('Argument Type Accuracy: 0.5882818685622464')\n",
        "  print('Argument Role Accuracy: 0.45764053839701')\n",
        "  print('Macro f-score: 0.305')\n",
        "\n",
        "\n",
        "def train_model(model_id, train_samples, dev_samples, best_model_file):\n",
        "    train_size = len(train_samples)\n",
        "    batch_count = int(math.ceil(train_size/batch_size))\n",
        "    move_last_batch = False\n",
        "    \n",
        "    if len(train_samples) - batch_size * (batch_count - 1) == 1:\n",
        "        move_last_batch = True\n",
        "        batch_count -= 1\n",
        "    \n",
        "    custom_print(batch_count)\n",
        "\n",
        "    # model = get_model(model_id)\n",
        "    model = SeqToSeqModel()\n",
        "\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    custom_print('Parameters size:', pytorch_total_params)\n",
        "\n",
        "    custom_print(model)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    if n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    criterion = nn.NLLLoss(ignore_index=0)\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    custom_print(optimizer)\n",
        "\n",
        "    best_dev_acc = -1.0\n",
        "    best_epoch_idx = -1\n",
        "    best_epoch_seed = -1\n",
        "    \n",
        "\n",
        "    for epoch_idx in range(0, num_epoch):\n",
        "        model.train()\n",
        "        model.zero_grad()\n",
        "\n",
        "        custom_print('Epoch:', epoch_idx + 1)\n",
        "        cur_seed = random_seed + epoch_idx + 1\n",
        "        set_random_seeds(cur_seed)\n",
        "\n",
        "        cur_shuffled_train_data = shuffle_data(train_samples)\n",
        "\n",
        "        start_time = datetime.datetime.now()\n",
        "        train_loss_val = 0.0\n",
        "\n",
        "        for batch_idx in tqdm(range(0, batch_count)):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = min(len(cur_shuffled_train_data), batch_start + batch_size)\n",
        "\n",
        "            if batch_idx == batch_count - 1 and move_last_batch:\n",
        "                batch_end = len(cur_shuffled_train_data)\n",
        "\n",
        "            cur_batch = cur_shuffled_train_data[batch_start:batch_end]\n",
        "            cur_samples_input = get_batch_data(cur_batch, True)\n",
        "\n",
        "            # np arrays to tensors\n",
        "            src_words_seq = torch.from_numpy(cur_samples_input['src_words'].astype('long'))\n",
        "            src_words_mask = torch.from_numpy(cur_samples_input['src_words_mask'].astype('uint8'))\n",
        "            trg_vocab_mask = torch.from_numpy(cur_samples_input['trg_vocab_mask'].astype('uint8'))\n",
        "            trg_words_seq = torch.from_numpy(cur_samples_input['trg_words'].astype('long'))\n",
        "            adj = torch.from_numpy(cur_samples_input['adj'].astype('float32'))\n",
        "            src_chars_seq = torch.from_numpy(cur_samples_input['src_chars'].astype('long'))\n",
        "\n",
        "            target = torch.from_numpy(cur_samples_input['target'].astype('long'))\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                src_words_seq = src_words_seq.cuda()\n",
        "                src_words_mask = src_words_mask.cuda()\n",
        "                trg_vocab_mask = trg_vocab_mask.cuda()\n",
        "                trg_words_seq = trg_words_seq.cuda()\n",
        "                adj = adj.cuda()\n",
        "                src_chars_seq = src_chars_seq.cuda()\n",
        "\n",
        "                target = target.cuda()\n",
        "\n",
        "            src_words_seq = autograd.Variable(src_words_seq)\n",
        "            src_words_mask = autograd.Variable(src_words_mask)\n",
        "            trg_vocab_mask = autograd.Variable(trg_vocab_mask)\n",
        "            trg_words_seq = autograd.Variable(trg_words_seq)\n",
        "            adj = autograd.Variable(adj)\n",
        "            src_chars_seq = autograd.Variable(src_chars_seq)\n",
        "\n",
        "            target = autograd.Variable(target)\n",
        "\n",
        "            outputs = model(src_words_seq, src_chars_seq, src_words_mask, trg_words_seq, trg_vocab_mask, adj, True)\n",
        "\n",
        "            target = target.view(-1, 1).squeeze()\n",
        "            loss = criterion(outputs, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
        "\n",
        "            if (batch_idx + 1) % update_freq == 0:\n",
        "                optimizer.step()\n",
        "                model.zero_grad()\n",
        "\n",
        "            train_loss_val += loss.item()\n",
        "\n",
        "        train_loss_val /= batch_count\n",
        "        end_time = datetime.datetime.now()\n",
        "        custom_print('Training loss:', train_loss_val)\n",
        "        custom_print('Training time:', end_time - start_time)\n",
        "\n",
        "        custom_print('\\nDev Results\\n')\n",
        "        set_random_seeds(random_seed)\n",
        "        dev_preds, dev_attns = predict(dev_samples, model, model_id)\n",
        "        \n",
        "        write_test_res(dev_samples, dev_preds, dev_attns, os.path.join(trg_data_folder, 'dev.out'))\n",
        "\n",
        "        ref_lines = open(trg_dev_file).read().splitlines()\n",
        "        pred_lines = open(os.path.join(trg_data_folder, 'dev.out')).read().splitlines()\n",
        "        event_lines = open(events_file).read().splitlines()\n",
        "        argument_lines = open(arguments_file).read().splitlines()\n",
        "        roles_lines = open(roles_file).read().splitlines()\n",
        "\n",
        "        dev_acc = calculate_f1(ref_lines, pred_lines, event_lines, argument_lines, roles_lines)\n",
        "\n",
        "\n",
        "        # pred_pos, gt_pos, correct_pos = get_F1(dev_samples, dev_preds, dev_attns)\n",
        "        # custom_print(pred_pos, '\\t', gt_pos, '\\t', correct_pos)\n",
        "        # p = float(correct_pos) / (pred_pos + 1e-8)\n",
        "        # r = float(correct_pos) / (gt_pos + 1e-8)\n",
        "        # dev_acc = (2 * p * r) / (p + r + 1e-8)\n",
        "        # custom_print('F1:', dev_acc)\n",
        "\n",
        "        if dev_acc >= best_dev_acc:\n",
        "            best_epoch_idx = epoch_idx + 1\n",
        "            best_epoch_seed = cur_seed\n",
        "            custom_print('model saved......')\n",
        "            best_dev_acc = dev_acc\n",
        "            torch.save(model.state_dict(), best_model_file)\n",
        "\n",
        "        custom_print('\\n\\n')\n",
        "        if epoch_idx + 1 - best_epoch_idx >= early_stop_cnt:\n",
        "            break\n",
        "    \n",
        "\n",
        "    custom_print('*******')\n",
        "    custom_print('Best Epoch:', best_epoch_idx)\n",
        "    custom_print('Best Epoch Seed:', best_epoch_seed)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_whannV3pB7N",
        "outputId": "e91d58d9-6cae-4692-bb55-1f186c7b2785"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "    random_seed = 42\n",
        "    src_data_folder = path\n",
        "    trg_data_folder = path + 'Deba_BERT_(64,75,100,50)'\n",
        "    job_mode = 'train'\n",
        "    embedding_type = 'w2v'\n",
        "    granular_mode = 1\n",
        "\n",
        "\n",
        "    \n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    set_random_seeds(random_seed)\n",
        "\n",
        "    # if not os.path.exists(trg_data_folder):\n",
        "    #     os.mkdir(trg_data_folder)\n",
        "    model_name = 1\n",
        "\n",
        "    #Tunable Hyperparameters\n",
        "\n",
        "    batch_size = 32\n",
        "    num_epoch = 100\n",
        "    max_src_len = 140\n",
        "    max_trg_len = 70\n",
        "    \n",
        "    fg()\n",
        "\n",
        "\n",
        "    if embedding_type == 'w2v':\n",
        "        embedding_file = os.path.join(src_data_folder, 'w2v.txt')\n",
        "    else:\n",
        "        embedding_file = os.path.join(src_data_folder, 'Bert_embeddings.txt')\n",
        "\n",
        "    update_freq = 1\n",
        "    enc_type = ['LSTM', 'GCN', 'LSTM-GCN'][0]\n",
        "    att_type = ['None', 'Unigram', 'N-Gram-Enc'][1]\n",
        "\n",
        "    copy_on = True\n",
        "\n",
        "    gcn_num_layers = 3\n",
        "\n",
        "    if embedding_type == 'w2v':\n",
        "        word_embed_dim = 300\n",
        "    else:\n",
        "        word_embed_dim = 768\n",
        "    \n",
        "    word_min_freq = 2\n",
        "    char_embed_dim = 50\n",
        "    char_feature_size = 50\n",
        "    conv_filter_size = 3\n",
        "    max_word_len = 10\n",
        "\n",
        "    enc_inp_size = word_embed_dim + char_feature_size\n",
        "    enc_hidden_size = word_embed_dim\n",
        "    dec_inp_size = enc_hidden_size\n",
        "    dec_hidden_size = dec_inp_size\n",
        "\n",
        "    drop_rate = 0.3\n",
        "    layers = 1\n",
        "    early_stop_cnt = 20\n",
        "    sample_cnt = 0\n",
        "    Sample = recordclass(\"Sample\", \"Id SrcLen SrcWords TrgLen TrgWords\")\n",
        "\n",
        "    events_file = os.path.join(src_data_folder, 'event_types.txt')\n",
        "    arguments_file = os.path.join(src_data_folder, 'arguments.txt')\n",
        "    roles_file = os.path.join(src_data_folder, 'roles.txt')\n",
        "\n",
        "    # events = get_relations(events_file)\n",
        "    # arguments = get_relations(arguments_file)\n",
        "    # roles = get_relations(roles_file)\n",
        "\n",
        "\n",
        "    # train a model\n",
        "    if job_mode == 'train':\n",
        "        # logger = open(os.path.join(trg_data_folder, 'training.log'), 'w')\n",
        "        # custom_print(sys.argv)\n",
        "        # custom_print(max_src_len, max_trg_len, drop_rate, layers)\n",
        "        # custom_print('loading data......')\n",
        "        model_file_name = os.path.join(trg_data_folder, 'model.h5py')\n",
        "        src_train_file = os.path.join(src_data_folder, 'train.sent')\n",
        "        trg_train_file = os.path.join(src_data_folder, 'train.tup')\n",
        "        # train_data = read_data(src_train_file, trg_train_file, 1)\n",
        "\n",
        "        src_dev_file = os.path.join(src_data_folder, 'dev.sent')\n",
        "        trg_dev_file = os.path.join(src_data_folder, 'dev.tup')\n",
        "        # dev_data = read_data(src_dev_file, trg_dev_file, 2)\n",
        "\n",
        "        # custom_print('Training data size:', len(train_data))\n",
        "        # custom_print('Development data size:', len(dev_data))\n",
        "\n",
        "        # custom_print(\"preparing vocabulary......\")\n",
        "        save_vocab = os.path.join(trg_data_folder, 'vocab.pkl')\n",
        "        # word_vocab, rev_word_vocab, char_vocab, word_embed_matrix = build_vocab(train_data, events, arguments, roles, save_vocab,\n",
        "        #                                                                         embedding_file)\n",
        "\n",
        "        # custom_print(\"Training started......\")\n",
        "        # train_model(model_name, train_data, dev_data, model_file_name)\n",
        "        # logger.close()\n",
        "    # job_mode = 'test'\n",
        "    if job_mode == 'test':\n",
        "        logger = open(os.path.join(trg_data_folder, 'test.log'), 'w')\n",
        "        custom_print(sys.argv)\n",
        "        custom_print(\"loading word vectors......\")\n",
        "        vocab_file_name = os.path.join(trg_data_folder, 'vocab.pkl')\n",
        "        word_vocab, char_vocab = load_vocab(vocab_file_name)\n",
        "\n",
        "        rev_word_vocab = OrderedDict()\n",
        "        for word in word_vocab:\n",
        "            idx = word_vocab[word]\n",
        "            rev_word_vocab[idx] = word\n",
        "\n",
        "        word_embed_matrix = np.zeros((len(word_vocab), word_embed_dim), dtype=np.float32)\n",
        "        custom_print('vocab size:', len(word_vocab))\n",
        "\n",
        "        src_test_file = os.path.join(src_data_folder, 'test.sent')\n",
        "        trg_test_file = os.path.join(src_data_folder, 'test.tup')\n",
        "        test_data = read_data(src_test_file, trg_test_file, 3)\n",
        "\n",
        "        custom_print('Test data size:', len(test_data))\n",
        "\n",
        "        custom_print('seed:', random_seed)\n",
        "        model_file = os.path.join(trg_data_folder, 'model.h5py')\n",
        "\n",
        "        best_model = get_model(model_name)\n",
        "        custom_print(best_model)\n",
        "        if torch.cuda.is_available():\n",
        "            best_model.cuda()\n",
        "        if n_gpu > 1:\n",
        "            best_model = torch.nn.DataParallel(best_model)\n",
        "        best_model.load_state_dict(torch.load(model_file))\n",
        "\n",
        "        custom_print('\\nTest Results\\n')\n",
        "        set_random_seeds(random_seed)\n",
        "        test_preds, test_attns = predict(test_data, best_model, model_name)\n",
        "\n",
        "        custom_print('Copy On')\n",
        "        write_test_res(test_data, test_preds, test_attns, os.path.join(trg_data_folder, 'test.out'))\n",
        "\n",
        "        # ref_lines = open(trg_test_file).readlines()\n",
        "        # pred_lines = open(os.path.join(trg_data_folder, 'test.out')).readlines()\n",
        "        # event_lines = open(events_file).readlines()\n",
        "        # argument_lines = open(arguments_file).readlines()\n",
        "        # roles_lines = open(roles_file).readlines()\n",
        "\n",
        "        ref_lines = open(trg_test_file).read().splitlines()\n",
        "        pred_lines = open(os.path.join(trg_data_folder, 'test.out')).read().splitlines()\n",
        "        event_lines = open(events_file).read().splitlines()\n",
        "        argument_lines = open(arguments_file).read().splitlines()\n",
        "        roles_lines = open(roles_file).read().splitlines()\n",
        "\n",
        "        mode = 1\n",
        "        custom_print('Overall F1')\n",
        "        # custom_print(cal_f1(ref_lines, pred_lines, event_lines, argument_lines, roles_lines, mode))\n",
        "        calculate_f1(ref_lines, pred_lines, event_lines, argument_lines, roles_lines)\n",
        "\n",
        "        copy_on = False\n",
        "        custom_print('Copy Off')\n",
        "        set_random_seeds(random_seed)\n",
        "        test_preds, test_attns = predict(test_data, best_model, model_name)\n",
        "        write_test_res(test_data, test_preds, test_attns, os.path.join(trg_data_folder, 'test_without_copy.out'))\n",
        "\n",
        "        # ref_lines = open(trg_test_file).readlines()\n",
        "        # pred_lines = open(os.path.join(trg_data_folder, 'test_without_copy.out')).readlines()\n",
        "        # event_lines = open(events_file).readlines()\n",
        "        # argument_lines = open(arguments_file).readlines()\n",
        "        # roles_lines = open(roles_file).readlines()\n",
        "\n",
        "        ref_lines = open(trg_test_file).read().splitlines()\n",
        "        pred_lines = open(os.path.join(trg_data_folder, 'test_without_copy.out')).read().splitlines()\n",
        "        event_lines = open(events_file).read().splitlines()\n",
        "        argument_lines = open(arguments_file).read().splitlines()\n",
        "        roles_lines = open(roles_file).read().splitlines()\n",
        "\n",
        "        mode = 1\n",
        "        custom_print('Overall F1')\n",
        "        # custom_print(cal_f1(ref_lines, pred_lines, event_lines, argument_lines, roles_lines, mode))\n",
        "        calculate_f1(ref_lines, pred_lines, event_lines, argument_lines, roles_lines)\n",
        "        logger.close()\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partwise Results\n",
            "Event Trigger Word Accuracy: 0.5535851966032878\n",
            "Event Type Accuracy: 0.6746337702338117\n",
            "Argument Identification Accuracy: 0.11955661124212545\n",
            "Argument Type Accuracy: 0.5882818685622464\n",
            "Argument Role Accuracy: 0.45764053839701\n",
            "Macro f-score: 0.305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuW_jTn9qmpI"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}